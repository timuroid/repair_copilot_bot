import os
from openai import OpenAI
from dotenv import load_dotenv

# Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° Ð¿ÐµÑ€ÐµÐ¼ÐµÐ½Ð½Ñ‹Ñ… Ð¾ÐºÑ€ÑƒÐ¶ÐµÐ½Ð¸Ñ
load_dotenv()

# ÐŸÐ¾Ð´ÐºÐ»ÑŽÑ‡ÐµÐ½Ð¸Ðµ Ðº OpenAI
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
if not OPENAI_API_KEY:
    raise ValueError("âŒ API-ÐºÐ»ÑŽÑ‡ OpenAI Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½! Ð£ÐºÐ°Ð¶Ð¸Ñ‚Ðµ ÐµÐ³Ð¾ Ð² .env")

client = OpenAI(api_key=OPENAI_API_KEY)

# Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° Ð¿Ñ€Ð¾Ð¼Ñ‚Ð¾Ð²
def load_prompt(filename: str) -> str:
    base_dir = os.path.abspath(os.path.dirname(__file__))
    prompt_path = os.path.join(base_dir, "../prompts", filename)
    with open(prompt_path, "r", encoding="utf-8") as f:
        return f.read()

SYSTEM_PROMPT_TEMPLATE = load_prompt("system_prompt.md")
HYPOTHESIS_SYSTEM_PROMPT = load_prompt("hypothesis_prompt.md")
SUMMARY_PROMPT_TEMPLATE = load_prompt("summary_prompt.md")

PROMPT_TEMPLATE = """


**Ð˜ÑÑ‚Ð¾Ñ€Ð¸Ñ Ð´Ð¸Ð°Ð»Ð¾Ð³Ð°:**
{history}

**ÐŸÐ¾ÑÐ»ÐµÐ´Ð½ÐµÐµ ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ðµ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»Ñ:**
'{user_message}'   

ðŸ§© **Ð“Ð¸Ð¿Ð¾Ñ‚ÐµÐ·Ñ‹ Ð¾Ñ‚ Ð²Ñ‚Ð¾Ñ€Ð¾Ð³Ð¾ Ð³ÐµÐ½ÐµÑ€Ð°Ñ‚Ð¾Ñ€Ð° (Ð´Ð»Ñ ÑÐ¿Ñ€Ð°Ð²ÐºÐ¸):**
{hypotheses}
"""

def generate_hypotheses(history: str, user_message: str) -> str:
    prompt = f"""
ðŸ“‚ **Ð˜ÑÑ‚Ð¾Ñ€Ð¸Ñ Ð´Ð¸Ð°Ð»Ð¾Ð³Ð° (ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚):**
{history}

ðŸ‘· **ÐŸÐ¾ÑÐ»ÐµÐ´Ð½ÐµÐµ ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ðµ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»Ñ:**
"{user_message}"

ðŸ“Œ Ð’ÐÐ–ÐÐž:
- Ð¡Ð½Ð°Ñ‡Ð°Ð»Ð° Ð¾Ð¿Ñ€ÐµÐ´ÐµÐ»Ð¸, Ð½Ð° ÐºÐ°ÐºÐ¾Ð¼ ÑÑ‚Ð°Ð¿Ðµ Ð°Ð½Ð°Ð»Ð¸Ð·Ð° ÑÐµÐ¹Ñ‡Ð°Ñ Ð½Ð°Ñ…Ð¾Ð´Ð¸Ñ‚ÑÑ Ð´Ð¸Ð°Ð»Ð¾Ð³ (Ñ‚ÐµÑ…Ð½Ð¸Ñ‡ÐµÑÐºÐ¸Ð¹, Ð²Ð½ÐµÑˆÐ½Ð¸Ð¹, Ð¾Ñ€Ð³Ð°Ð½Ð¸Ð·Ð°Ñ†Ð¸Ð¾Ð½Ð½Ñ‹Ð¹ Ð¸Ð»Ð¸ ÑƒÐ¿Ñ€Ð°Ð²Ð»ÐµÐ½Ñ‡ÐµÑÐºÐ¸Ð¹).
- Ð¡Ð³ÐµÐ½ÐµÑ€Ð¸Ñ€ÑƒÐ¹ 10 Ñ€Ð°Ð·Ð½Ð¾Ð¾Ð±Ñ€Ð°Ð·Ð½Ñ‹Ñ… Ð³Ð¸Ð¿Ð¾Ñ‚ÐµÐ·, ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ðµ ÑÐ¾Ð¾Ñ‚Ð²ÐµÑ‚ÑÑ‚Ð²ÑƒÑŽÑ‚ ÑÑ‚Ð¾Ð¼Ñƒ ÑÑ‚Ð°Ð¿Ñƒ.
- Ð“Ð¸Ð¿Ð¾Ñ‚ÐµÐ·Ñ‹ Ð¿Ð¸ÑˆÐ¸ ÐºÑ€Ð°Ñ‚ÐºÐ¾, Ñ Ð½Ñ€ÐµÐ±Ð¾Ð»ÑŒÑˆÐ¸Ð¼Ð¸ Ð¿Ð¾ÑÑÐ½ÐµÐ½Ð¸ÑÐ¼Ð¸
"""

    response = client.chat.completions.create(
        model="gpt-4o",
        messages=[
            {"role": "system", "content": HYPOTHESIS_SYSTEM_PROMPT},
            {"role": "user", "content": prompt}
        ],
        temperature=0.6,
    )

    return response.choices[0].message.content.strip()

def generate_response(user_id: int, history: list[tuple[str, str]], user_message: str) -> str:
    formatted_history = "\n".join([f"ðŸ‘· {msg}\nðŸ¤– {resp}" for msg, resp in history])
    hypotheses = generate_hypotheses(formatted_history, user_message)
    prompt = PROMPT_TEMPLATE.format(history=formatted_history, user_message=user_message, hypotheses=hypotheses)

    response = client.chat.completions.create(
        model="gpt-4o",
        messages=[
            {"role": "system", "content": SYSTEM_PROMPT_TEMPLATE},
            {"role": "user", "content": prompt}
        ],
        temperature=0.6,
    )
    return response.choices[0].message.content

def generate_summary(messages: list[tuple]) -> str:
    formatted_history = "\n".join([f"ðŸ‘· {msg}\nðŸ¤– {resp}" for (_, msg, resp, _, _) in messages])
    summary_prompt = SUMMARY_PROMPT_TEMPLATE.format(messages=formatted_history)

    response = client.chat.completions.create(
        model="gpt-4o",
        messages=[{"role": "system", "content": summary_prompt}],
    )
    return response.choices[0].message.content
