import os
from openai import OpenAI
import sqlite3
import logging
from dotenv import load_dotenv
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from datetime import datetime
from openai._exceptions import OpenAIError, RateLimitError
from app import db


# –ó–∞–≥—Ä—É–∂–∞–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è –∏–∑ .env
load_dotenv()

# –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º API-–∫–ª—é—á–∏
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
if not OPENAI_API_KEY:
    raise ValueError("‚ùå API-–∫–ª—é—á OpenAI –Ω–µ –Ω–∞–π–¥–µ–Ω! –£–∫–∞–∂–∏—Ç–µ –µ–≥–æ –≤ —Ñ–∞–π–ª–µ .env")

client = OpenAI(
    base_url="https://openrouter.ai/api/v1",
    api_key=OPENAI_API_KEY,
    )

app = FastAPI()

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    handlers=[
        logging.FileHandler("bot.log", encoding="utf-8"),
        logging.StreamHandler()
    ]
)




db.init_db()

class UserMessage(BaseModel):
    user_id: int
    message: str

def load_prompt(filename: str) -> str:
    base_dir = os.path.abspath(os.path.dirname(__file__))
    prompt_path = os.path.join(base_dir, "prompts", filename)
    with open(prompt_path, "r", encoding="utf-8") as f:
        return f.read()


SYSTEM_PROMPT_TEMPLATE = load_prompt("system_prompt.md")
HYPOTHESIS_SYSTEM_PROMPT = load_prompt("hypothesis_prompt.md")
SUMMARY_PROMPT_TEMPLATE = load_prompt("summary_prompt.md")
PROMPT_TEMPLATE = """


**–ò—Å—Ç–æ—Ä–∏—è –¥–∏–∞–ª–æ–≥–∞:**
{history}

**–ü–æ—Å–ª–µ–¥–Ω–µ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è:**
'{user_message}'   

üß© **–ì–∏–ø–æ—Ç–µ–∑—ã –æ—Ç –≤—Ç–æ—Ä–æ–≥–æ –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–∞ (–¥–ª—è —Å–ø—Ä–∞–≤–∫–∏):**
{hypotheses}
"""


def generate_hypotheses(history: str, user_message: str) -> str:
    prompt = f"""
üóÇ **–ò—Å—Ç–æ—Ä–∏—è –¥–∏–∞–ª–æ–≥–∞ (–∫–æ–Ω—Ç–µ–∫—Å—Ç):**
{history}

üë∑ **–ü–æ—Å–ª–µ–¥–Ω–µ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è:**
"{user_message}"

üìç –í–ê–ñ–ù–û:
- –°–Ω–∞—á–∞–ª–∞ –æ–ø—Ä–µ–¥–µ–ª–∏, –Ω–∞ –∫–∞–∫–æ–º —ç—Ç–∞–ø–µ –∞–Ω–∞–ª–∏–∑–∞ —Å–µ–π—á–∞—Å –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –¥–∏–∞–ª–æ–≥ (—Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π, –≤–Ω–µ—à–Ω–∏–π, –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–æ–Ω–Ω—ã–π –∏–ª–∏ —É–ø—Ä–∞–≤–ª–µ–Ω—á–µ—Å–∫–∏–π).
- –°–≥–µ–Ω–µ—Ä–∏—Ä—É–π 10 —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã—Ö –≥–∏–ø–æ—Ç–µ–∑, –∫–æ—Ç–æ—Ä—ã–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—Ç —ç—Ç–æ–º—É —ç—Ç–∞–ø—É.
- –ì–∏–ø–æ—Ç–µ–∑—ã –ø–∏—à–∏ –∫—Ä–∞—Ç–∫–æ, —Å –Ω—Ä–µ–±–æ–ª—å—à–∏–º–∏ –ø–æ—è—Å–Ω–µ–Ω–∏—è–º–∏–π
"""

    response = client.chat.completions.create(
        model="deepseek/deepseek-chat-v3-0324",
        messages=[{"role": "system", "content": HYPOTHESIS_SYSTEM_PROMPT},
                  {"role": "user", "content": prompt}],
        temperature=0.6,
        
    )

    return response.choices[0].message.content.strip()



def get_gpt_response(user_id: int, message: str) -> str:
    history = db.fetch_dialog_history(user_id)
    formatted_history = "\n".join([f"üë∑ {msg}\nü§ñ {resp}" for msg, resp in history])

    hypotheses = generate_hypotheses(formatted_history, message)
    print("\nüß† –ü–æ–¥—Å–∫–∞–∑–∫–∏ –æ—Ç –∞–Ω–∞–ª–∏—Ç–∏–∫–∞:\n" + hypotheses + "\n" + "-"*50)

    prompt = PROMPT_TEMPLATE.format(history=formatted_history, user_message=message, hypotheses=hypotheses)

    try:
        response = client.chat.completions.create(
            model="deepseek/deepseek-chat-v3-0324",
            messages=[
                {"role": "system", "content": SYSTEM_PROMPT_TEMPLATE},
                {"role": "user", "content": prompt}
            ],
            temperature=0.6,
        )
        bot_reply = response.choices[0].message.content
        db.save_dialog_entry(user_id, message, bot_reply)
        return bot_reply
    except RateLimitError as e:
        logging.error(f"üö¶ –ü—Ä–µ–≤—ã—à–µ–Ω –ª–∏–º–∏—Ç: {e}")
        raise HTTPException(status_code=429, detail="–õ–∏–º–∏—Ç OpenAI –ø—Ä–µ–≤—ã—à–µ–Ω.")
    except OpenAIError as e:
        logging.error(f"‚ùå –û—à–∏–±–∫–∞ OpenAI: {e}")
        raise HTTPException(status_code=500, detail="–û—à–∏–±–∫–∞ OpenAI.")
    except Exception as e:
        logging.error(f"‚ö†Ô∏è –û–±—â–∞—è –æ—à–∏–±–∫–∞: {e}")
        raise HTTPException(status_code=500, detail="–ù–µ–ø—Ä–µ–¥–≤–∏–¥–µ–Ω–Ω–∞—è –æ—à–∏–±–∫–∞.")

@app.post("/chat")
def chat_with_bot(user_message: UserMessage):
    return {"response": get_gpt_response(user_message.user_id, user_message.message)}

@app.get("/check_dialog")
def check_dialog(user_id: int):
    return {"status": db.check_dialog_status(user_id)}

@app.post("/end_dialog")
def end_dialog(user_id: int):
    messages = db.finish_dialog(user_id)
    if not messages:
        return {"error": "–ù–µ—Ç –∑–∞–≤–µ—Ä—à—ë–Ω–Ω—ã—Ö –¥–∏–∞–ª–æ–≥–æ–≤"}

    # üëá –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –∏—Å—Ç–æ—Ä–∏—é –¥–∏–∞–ª–æ–≥–∞ –≤ —á–∏—Ç–∞–µ–º—ã–π —Ç–µ–∫—Å—Ç
    formatted_history = "\n".join([
        f"üë∑ {msg}\nü§ñ {resp}" for (_, msg, resp, _, _) in messages
    ])

    # üëá –ü–æ–¥—Å—Ç–∞–≤–ª—è–µ–º –∏—Å—Ç–æ—Ä–∏—é –≤ —à–∞–±–ª–æ–Ω
    summary_prompt = SUMMARY_PROMPT_TEMPLATE.format(messages=formatted_history)

    try:
        response = client.chat.completions.create(
            model="deepseek/deepseek-chat-v3-0324",
            messages=[{"role": "system", "content": summary_prompt}],
        )
        summary = response.choices[0].message.content
    except OpenAIError as e:
        logging.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å–≤–æ–¥–∫–∏: {e}")
        summary = f"–û—à–∏–±–∫–∞ OpenAI: {str(e)}"
    except Exception as e:
        logging.error(f"‚ö†Ô∏è –û–±—â–∞—è –æ—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å–≤–æ–¥–∫–∏: {e}")
        summary = f"–û—à–∏–±–∫–∞: {str(e)}"

    return {"message": "–î–∏–∞–ª–æ–≥ –∑–∞–≤–µ—Ä—à—ë–Ω", "summary": summary}

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000, workers=2)
